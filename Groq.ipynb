{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low latency is crucial for Large Language Models (LLMs) because it directly impacts the user experience, model performance, and overall efficiency of language-based applications. Here are some reasons why low latency is essential for LLMs:\\n\\n1. **Real-time Interaction**: LLMs are often used in applications that require real-time interaction, such as chatbots, virtual assistants, and language translation systems. Low latency ensures that the model responds quickly to user input, providing a seamless and engaging experience.\\n2. **Conversational Flow**: In conversational AI, latency can disrupt the natural flow of conversation. High latency can lead to awkward pauses, making the interaction feel unnatural and frustrating. Low latency helps maintain a smooth conversation, allowing users to engage more naturally with the model.\\n3. **Model Performance**: LLMs rely on complex algorithms and massive datasets to generate responses. High latency can lead to increased computational overhead, which can negatively impact model performance, accuracy, and reliability. Low latency enables the model to process requests efficiently, reducing the risk of errors and inaccuracies.\\n4. **Scalability**: As the number of users and requests increases, high latency can become a bottleneck, limiting the scalability of LLM-based applications. Low latency enables the model to handle a higher volume of requests, making it more suitable for large-scale deployments.\\n5. **User Experience**: High latency can lead to user frustration, abandonment, and a negative overall experience. Low latency ensures that users receive prompt responses, which is essential for building trust, satisfaction, and loyalty.\\n6. **Competitive Advantage**: In today's fast-paced digital landscape, low latency can be a key differentiator for LLM-based applications. By providing rapid responses, developers can gain a competitive edge, attract more users, and establish a strong market presence.\\n7. **Edge Computing**: With the increasing adoption of edge computing, low latency becomes even more critical. Edge computing involves processing data closer to the user, reducing latency and improving real-time performance. LLMs optimized for low latency can take full advantage of edge computing benefits.\\n8. **Safety-Critical Applications**: In safety-critical applications, such as autonomous vehicles or medical diagnosis, high latency can have severe consequences. Low latency ensures that critical information is processed and responded to quickly, reducing the risk of accidents or misdiagnosis.\\n9. **Cost Savings**: High latency can result in increased infrastructure costs, as more resources are required to handle the additional computational overhead. Low latency enables developers to optimize their infrastructure, reducing costs and improving overall efficiency.\\n10. **Future-Proofing**: As LLMs continue to evolve and become more sophisticated, low latency will become even more essential. By optimizing for low latency today, developers can future-proof their applications, ensuring they remain competitive and effective in the years to come.\\n\\nIn summary, low latency is vital for LLMs because it enables real-time interaction, conversational flow, model performance, scalability, user experience, competitive advantage, edge computing, safety-critical applications, cost savings, and future-proofing. By prioritizing low latency, developers can create more efficient, effective, and engaging language-based applications that meet the evolving needs of users.\", response_metadata={'token_usage': {'completion_tokens': 644, 'prompt_tokens': 33, 'total_tokens': 677, 'completion_time': 1.955605524, 'prompt_time': 0.006300268, 'queue_time': None, 'total_time': 1.961905792}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None}, id='run-52c84f66-7f6a-4db7-8fd6-231b8fa166b8-0', usage_metadata={'input_tokens': 33, 'output_tokens': 644, 'total_tokens': 677})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    # api_key=\"\" # Optional if not set as an environment variable\n",
    ")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"text\": \"Explain the importance of low latency for LLMs.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_weather',\n",
       "  'args': {'location': 'San Francisco', 'unit': 'Celsius'},\n",
       "  'id': 'call_gkh6',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'get_current_weather',\n",
       "  'args': {'location': 'Tokyo', 'unit': 'Celsius'},\n",
       "  'id': 'call_e598',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_weather(location: str, unit: Optional[str]):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return \"Cloudy with a chance of rain.\"\n",
    "\n",
    "\n",
    "tool_model = chat.bind_tools(\n",
    "    [get_current_weather],\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "res = tool_model.invoke(\"What is the weather like in San Francisco and Tokyo?\")\n",
    "\n",
    "res.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!', rating=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There\\'s a bright ball of gas in the sky,\\nThat rises and sets, making spirits high.\\nIt gives us light and warmth,\\nOn sunny days it transforms,\\nThe world into a golden, cheery pi.\\n\\n(Note: I tried to make the last line \"The world into a golden, cheery high\", but that didn\\'t fit the rhythm of a limerick. So I changed it to \"pi\", which is a mathematical constant and a playful way to end the limerick.)', response_metadata={'token_usage': {'completion_tokens': 114, 'prompt_tokens': 18, 'total_tokens': 132, 'completion_time': 0.178125, 'prompt_time': 0.00215303, 'queue_time': None, 'total_time': 0.18027803}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-d6bf4469-9571-42e9-881f-92cf4b12423a-0', usage_metadata={'input_tokens': 18, 'output_tokens': 114, 'total_tokens': 132})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Write a Limerick about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "await chain.ainvoke(\n",
    "    {\n",
    "        \"topic\": \"The Sun\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent, luminous,\n",
      "Glowing in the velvet night,\n",
      "The Moon's gentle light."
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a haiku about {topic}\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream(\n",
    "    {\n",
    "        \"topic\": \"The Moon\",\n",
    "    }\n",
    "):\n",
    "    print(\n",
    "        chunk.content,\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"response\": \"The best bear is subjective and depends on personal preference. Some popular bears include the Teddy Bear, Grizzly Bear, and Panda Bear.\",\\n\"followup_question\": \"Do you have a specific type of bear in mind?\"\\n}', response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 63, 'total_tokens': 123, 'completion_time': 0.09375, 'prompt_time': 0.004336649, 'queue_time': None, 'total_time': 0.098086649}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-c9f98fbe-e52b-47cf-8c52-a8104ded81a2-0', usage_metadata={'input_tokens': 63, 'output_tokens': 60, 'total_tokens': 123})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    model_kwargs={\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_object\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Always respond with a JSON object with two string keys: \"response\" and \"followup_question\".\n",
    "\"\"\"\n",
    "human = \"{question}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what bear is best?\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
