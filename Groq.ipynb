{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low latency is crucial for Large Language Models (LLMs) because it enables them to process and respond to user input in a timely and efficient manner. Here are some reasons why low latency is important for LLMs:\\n\\n1. **Improved User Experience**: Low latency ensures that users can interact with the LLM quickly and seamlessly, without having to wait for responses. This is particularly important for applications like chatbots, virtual assistants, and language translation tools, where speed and responsiveness are critical.\\n2. **Increased Accuracy**: LLMs rely on complex algorithms and computations to generate responses. Low latency allows these models to process and analyze user input more quickly, which can lead to more accurate and relevant responses.\\n3. **Reduced Errors**: High latency can lead to errors and misunderstandings, as the LLM may not have enough time to process the user's input correctly. Low latency reduces the likelihood of errors and ensures that the LLM can respond accurately and consistently.\\n4. **Scalability**: As LLMs are deployed in large-scale applications, low latency becomes even more critical. It enables the models to handle a high volume of requests and responses without experiencing delays or slowdowns.\\n5. **Cost-Effectiveness**: Low latency can reduce the computational resources required to run LLMs, making them more cost-effective and energy-efficient. This is particularly important for cloud-based applications, where computing resources can be expensive.\\n6. **Real-Time Processing**: LLMs are designed to process and respond to user input in real-time. Low latency ensures that this processing occurs quickly and efficiently, enabling the LLM to respond to user queries and requests in a timely manner.\\n7. **Enhanced Conversational Flow**: Low latency enables LLMs to maintain a smooth and natural conversational flow, which is essential for applications like chatbots and virtual assistants. It allows the LLM to respond quickly and accurately, creating a more engaging and interactive experience for users.\\n\\nIn summary, low latency is essential for LLMs because it enables them to process and respond to user input quickly and accurately, improving the overall user experience, reducing errors, and increasing scalability and cost-effectiveness.\", response_metadata={'token_usage': {'completion_tokens': 438, 'prompt_tokens': 33, 'total_tokens': 471, 'completion_time': 0.347861265, 'prompt_time': 0.008946309, 'queue_time': None, 'total_time': 0.35680757399999996}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_33d61fdfc3', 'finish_reason': 'stop', 'logprobs': None}, id='run-e71e9cc4-f3e2-4b3d-8ddd-41c07d235216-0', usage_metadata={'input_tokens': 33, 'output_tokens': 438, 'total_tokens': 471})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-8b-8192\",\n",
    "    # api_key=\"\" # Optional if not set as an environment variable\n",
    ")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"text\": \"Explain the importance of low latency for LLMs.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_weather',\n",
       "  'args': {'location': 'San Francisco', 'unit': 'metric'},\n",
       "  'id': 'call_tpc9',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'get_current_weather',\n",
       "  'args': {'location': 'Tokyo', 'unit': 'metric'},\n",
       "  'id': 'call_v7nv',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_weather(location: str, unit: Optional[str]):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return \"Cloudy with a chance of rain.\"\n",
    "\n",
    "\n",
    "tool_model = chat.bind_tools(\n",
    "    [get_current_weather],\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "res = tool_model.invoke(\"What is the weather like in San Francisco and Tokyo?\")\n",
    "\n",
    "res.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!', rating=8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There\\'s a bright ball of gas in the sky,\\nThat rises and sets, making spirits high.\\nIt gives us light and warmth,\\nOn sunny days it transforms,\\nThe world into a golden, cheery pi.\\n\\n(Note: I tried to make the last line \"The world into a golden, cheery high\", but that didn\\'t fit the rhythm of a limerick. So I changed it to \"pi\", which is a mathematical constant and a playful way to end the limerick.)', response_metadata={'token_usage': {'completion_tokens': 114, 'prompt_tokens': 18, 'total_tokens': 132, 'completion_time': 0.178125, 'prompt_time': 0.002279339, 'queue_time': None, 'total_time': 0.180404339}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-59d15f52-3a82-4bd0-9599-b36b7ce5b445-0', usage_metadata={'input_tokens': 18, 'output_tokens': 114, 'total_tokens': 132})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Write a Limerick about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "await chain.ainvoke(\n",
    "    {\n",
    "        \"topic\": \"The Sun\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent, luminous,\n",
      "Glowing in the velvet night,\n",
      "The Moon's gentle light."
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a haiku about {topic}\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream(\n",
    "    {\n",
    "        \"topic\": \"The Moon\",\n",
    "    }\n",
    "):\n",
    "    print(\n",
    "        chunk.content,\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"response\": \"The best bear is subjective and depends on individual preferences. Some popular choices are the Teddy Bear and the Grizzly Bear.\",\\n\"followup_question\": \"Is there a specific type of bear you are interested in?\"\\n}', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 63, 'total_tokens': 120, 'completion_time': 0.0890625, 'prompt_time': 0.004143942, 'queue_time': None, 'total_time': 0.093206442}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-9f6f11e7-f368-43c1-8f02-2d7200b40e50-0', usage_metadata={'input_tokens': 63, 'output_tokens': 57, 'total_tokens': 120})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    model_kwargs={\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_object\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Always respond with a JSON object with two string keys: \"response\" and \"followup_question\".\n",
    "\"\"\"\n",
    "human = \"{question}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what bear is best?\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
