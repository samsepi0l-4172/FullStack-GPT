{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low latency is crucial for Large Language Models (LLMs) because it enables them to process and respond to user input in a timely and efficient manner. Here are some reasons why low latency is important for LLMs:\\n\\n1. **Improved User Experience**: Low latency ensures that users can interact with the LLM quickly and seamlessly, without having to wait for responses. This is particularly important for applications like chatbots, virtual assistants, and language translation tools, where speed and responsiveness are critical.\\n2. **Increased Accuracy**: LLMs rely on complex algorithms and computations to generate responses. Low latency allows these models to process and analyze user input more quickly, which can lead to more accurate and relevant responses.\\n3. **Reduced Errors**: High latency can lead to errors and misunderstandings, as the LLM may not have enough time to process the user's input correctly. Low latency reduces the likelihood of errors and ensures that the LLM can respond accurately and consistently.\\n4. **Scalability**: As LLMs are deployed in large-scale applications, low latency becomes even more critical. It enables the models to handle a high volume of requests and responses without experiencing delays or slowdowns.\\n5. **Cost-Effectiveness**: Low latency can reduce the computational resources required to run LLMs, making them more cost-effective and energy-efficient. This is particularly important for cloud-based applications, where computing resources can be expensive.\\n6. **Real-Time Processing**: LLMs are designed to process and respond to user input in real-time. Low latency ensures that this processing occurs quickly and efficiently, enabling the LLM to respond to user queries and requests in a timely manner.\\n7. **Enhanced Conversational Flow**: Low latency enables LLMs to maintain a smooth and natural conversational flow, which is essential for applications like chatbots and virtual assistants. It allows the LLM to respond quickly and accurately, creating a more engaging and interactive experience for users.\\n\\nIn summary, low latency is essential for LLMs because it enables them to process and respond to user input quickly and accurately, improving the overall user experience, reducing errors, and increasing scalability and cost-effectiveness.\", response_metadata={'token_usage': {'completion_tokens': 438, 'prompt_tokens': 33, 'total_tokens': 471, 'completion_time': 0.347417299, 'prompt_time': 0.007324909, 'queue_time': None, 'total_time': 0.354742208}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-a90e4c6a-18a5-4993-a19e-414dc47310ac-0', usage_metadata={'input_tokens': 33, 'output_tokens': 438, 'total_tokens': 471})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-8b-8192\",\n",
    "    # api_key=\"\" # Optional if not set as an environment variable\n",
    ")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"text\": \"Explain the importance of low latency for LLMs.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_weather',\n",
       "  'args': {'location': 'San Francisco', 'unit': 'metric'},\n",
       "  'id': 'call_mtat',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'get_current_weather',\n",
       "  'args': {'location': 'Tokyo', 'unit': 'metric'},\n",
       "  'id': 'call_2b5t',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_weather(location: str, unit: Optional[str]):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return \"Cloudy with a chance of rain.\"\n",
    "\n",
    "\n",
    "tool_model = chat.bind_tools(\n",
    "    [get_current_weather],\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "res = tool_model.invoke(\"What is the weather like in San Francisco and Tokyo?\")\n",
    "\n",
    "res.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!', rating=8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There once was a sun in the sky,\\nWhose warmth and bright light did not die,\\nIt shone with great might,\\nAnd lit up the night,\\nAnd brought joy to all who did fly.', response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 18, 'total_tokens': 60, 'completion_time': 0.032771902, 'prompt_time': 0.004121825, 'queue_time': None, 'total_time': 0.036893727}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}, id='run-13c41274-48ae-4b66-b899-ee6edbb62c1b-0', usage_metadata={'input_tokens': 18, 'output_tokens': 42, 'total_tokens': 60})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Write a Limerick about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "await chain.ainvoke(\n",
    "    {\n",
    "        \"topic\": \"The Sun\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silvery glow bright\n",
      "Moon's gentle light on my facevery glow bright\n",
      "Moon's gentle light on my face\n",
      "Peaceful night's delight"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a haiku about {topic}\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream(\n",
    "    {\n",
    "        \"topic\": \"The Moon\",\n",
    "    }\n",
    "):\n",
    "    print(\n",
    "        chunk.content,\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"response\": \"The best bear is a matter of personal opinion, but some popular species include the polar bear, brown bear, and black bear. Each has its own unique characteristics and adaptations that make it well-suited to its environment.\",\\n\"followup_question\": \"What do you think makes a bear \\'best\\'? Is it its size, strength, or something else?\"\\n}', response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 50, 'total_tokens': 128, 'completion_time': 0.061320272, 'prompt_time': 0.013660193, 'queue_time': None, 'total_time': 0.074980465}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_33d61fdfc3', 'finish_reason': 'stop', 'logprobs': None}, id='run-b0db1885-7cab-4cd4-929b-bec530c383b3-0', usage_metadata={'input_tokens': 50, 'output_tokens': 78, 'total_tokens': 128})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    model_kwargs={\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_object\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Always respond with a JSON object with two string keys: \"response\" and \"followup_question\".\n",
    "\"\"\"\n",
    "human = \"{question}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what bear is best?\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
